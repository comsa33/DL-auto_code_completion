{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "720f335d-161b-4a41-86ba-97f758506634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from dataset import DataSet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class auto_coding():\n",
    "    \n",
    "    def __init__(self, new_code, model_name, batch_size=50, epochs=100, patience=10, verbose=1, model_summary=False, model_save=False, model_load=False, data_split=False, data_split_num=5, model_load_only=False):\n",
    "        \n",
    "        tf.random.set_seed(42)\n",
    "        \n",
    "        # get data\n",
    "        self.data_ = DataSet(new_code=new_code)\n",
    "        self.x, self.y, self.max_word_len, self.vocab_size = self.data_.get_dataset()\n",
    "        self.word2idx, self.idx2word = self.data_.make_word2idx_idx2word(get_vocab_size=False)\n",
    "        self.y_pred = None\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # model parameters\n",
    "        self.new_code = new_code + ' ' + ' '.join(['tuple', 'list', 'dic', 'return', 'print', \n",
    "                         'for', 'range', 'while', 'not', 'is', 'sort'])\n",
    "        self.filepath = f\"{model_name}/best.hdf5\"\n",
    "        self.model_summary = model_summary\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.model_save = model_save\n",
    "        self.model_load = model_load\n",
    "        self.model_name = model_name\n",
    "        self.EPOCHS = epochs\n",
    "        self.patience = patience\n",
    "        self.data_split = data_split\n",
    "        self.data_split_num = data_split_num\n",
    "        \n",
    "        if model_load_only:\n",
    "            self.model = tf.keras.models.load_model(f'{self.model_name}/{self.model_name}.h5')\n",
    "            self.model.load_weights(self.filepath)\n",
    "            \n",
    "        else:\n",
    "            if self.data_split:\n",
    "                self.total_xys = self.split_data(num=self.data_split_num)\n",
    "\n",
    "                for x, y in self.total_xys:\n",
    "                    self.train_model(x, y)\n",
    "            else:\n",
    "                self.train_model(self.x, self.y)\n",
    "        \n",
    "    def train_model(self, x, y):\n",
    "        # model compiling\n",
    "        ## if model_load=True, load the model from a directory specified as model_name\n",
    "        if self.model_load:\n",
    "            try:\n",
    "                self.model = tf.keras.models.load_model(f'{self.model_name}/{self.model_name}.h5')\n",
    "                print(f\">>> Load {self.model_name} over your model!\")\n",
    "            except OSError:\n",
    "                self.model = self.gru_lstm_model()\n",
    "                print(f\">>> Failed to load {self.model_name}. \\n>>>Load base model instead!\")\n",
    "        else:\n",
    "            self.model = self.gru_lstm_model()\n",
    "            print(\"Build a new model before starting your training!\")\n",
    "        \n",
    "        # mode whether to show the summary or not\n",
    "        if self.model_summary:\n",
    "            print(self.model.summary())\n",
    "            \n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'], run_eagerly=True)\n",
    "        \n",
    "        # callback parameters\n",
    "        self.callback = tf.keras.callbacks.EarlyStopping(monitor='acc', patience=self.patience)\n",
    "        os.makedirs(self.model_name, exist_ok=True)\n",
    "        self.model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.filepath,\n",
    "                                                                           save_weights_only=True,\n",
    "                                                                           save_best_only=True,\n",
    "                                                                           monitor='acc', \n",
    "                                                                           save_freq='epoch')\n",
    "        \n",
    "        # train model\n",
    "        print(\"==== TRAINING START ====\")\n",
    "        self.history = self.model.fit(x, y, \n",
    "                                      batch_size = self.batch_size,\n",
    "                                      verbose = self.verbose,\n",
    "                                      epochs=self.EPOCHS, \n",
    "                                      callbacks=[self.callback, \n",
    "                                                 self.model_checkpoint_callback]\n",
    "                                      )\n",
    "        print(\"==== TRAINING DONE ====\")\n",
    "        \n",
    "        print(\"Now, Load the best weights on your model.\")\n",
    "        self.model.load_weights(self.filepath)\n",
    "        \n",
    "        # if model-save mode, save the current trained model\n",
    "        if self.model_save:\n",
    "            os.makedirs(self.model_name, exist_ok=True)\n",
    "            self.model.save(f'{self.model_name}/{self.model_name}.h5')\n",
    "            print(f\">>> Saved this trained model over {self.model_name}.h5 file!\")\n",
    "            \n",
    "    def gru_lstm_model(self):\n",
    "        inputs = tf.keras.layers.Input(shape=(self.max_word_len,))\n",
    "        embedding = tf.keras.layers.Embedding(self.vocab_size, 500, input_length=self.max_word_len)(inputs) # (None, max_word_len, 500)\n",
    "        x = tf.keras.layers.GRU(512)(embedding)\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        \n",
    "        x1 = tf.keras.layers.LSTM(512)(embedding)\n",
    "        x1 = tf.keras.layers.Dense(256, activation='relu')(x1)\n",
    "        \n",
    "        x = tf.keras.layers.concatenate([x, x1], axis=1)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(self.vocab_size, activation='softmax')(x) # (None, length of word dictionary)\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # return unregistered word input to '[UKN]' token to prevent KeyError/IndexError\n",
    "    def word2idx_get(self, char):\n",
    "        if self.word2idx.get(char) == None:\n",
    "            return self.word2idx.get('[UKN]')\n",
    "        else:\n",
    "            return self.word2idx.get(char)\n",
    "           \n",
    "    def vec2word(self, x):\n",
    "        x_ = []\n",
    "        for word in x:\n",
    "            word_temp = ''\n",
    "            for char in word:\n",
    "                if char != 0:\n",
    "                    word_temp += self.idx2word.get(char)\n",
    "            x_.append(word_temp)\n",
    "        return x_\n",
    "    \n",
    "    # predict the user's choice\n",
    "    def return_pred(self, test):\n",
    "        assert type(test) == list\n",
    "        test_vec = [[self.word2idx_get(char) for char in word] for word in test]\n",
    "        x_test = tf.keras.preprocessing.sequence.pad_sequences(test_vec, maxlen=self.max_word_len, padding='pre')\n",
    "        self.y_pred = self.model.predict(x_test)\n",
    "\n",
    "    # get all possible words with over 0.027 softmax values\n",
    "    def predict_recommendations(self):\n",
    "        best_preds = self.predict_best()\n",
    "        result_arr = np.argwhere(self.y_pred>0.027)\n",
    "        result = []\n",
    "        for pred_idx in range(len(self.y_pred)):\n",
    "            temp_ls = []\n",
    "            for i in range(len(result_arr)):\n",
    "                if pred_idx == result_arr[i, 0]:\n",
    "                    temp_ls.append(result_arr[i, -1])\n",
    "            result.append(temp_ls)\n",
    "\n",
    "        return [[self.idx2word.get(idx) for idx in pred] for pred in result]\n",
    "\n",
    "    # predict the most promising following word from given word fragments\n",
    "    def predict_best(self):\n",
    "        \"\"\"return the list of best suited words\"\"\"\n",
    "        pred = [np.argmax(result) for result in self.y_pred]\n",
    "        return [self.idx2word.get(idx) for idx in pred]\n",
    "\n",
    "    # display the result of recommended auto-completions\n",
    "    def auto_complete(self, test, display_result=True):\n",
    "        \"\"\"print matches of given word and recommended auto completion\"\"\"\n",
    "        if type(test) == str:\n",
    "            test = [test]\n",
    "        self.return_pred(test)\n",
    "        best_preds = self.predict_best()\n",
    "        recc_preds = self.predict_recommendations()\n",
    "        \n",
    "        self.results = []\n",
    "        self.best_results = []\n",
    "        for i in range(len(test)):\n",
    "            self.results.append([test[i]+candidate for candidate in recc_preds[i]])\n",
    "            self.best_results.append(test[i]+best_preds[i])\n",
    "            if display_result:\n",
    "                print(test[i], ' - best match :', test[i]+best_preds[i])\n",
    "                print('\\t - all recommendations : ', [test[i]+candidate for candidate in recc_preds[i]])\n",
    "                \n",
    "        # calculate AFR : Accuracy for Recommendations\n",
    "        bow = self.word2idx.keys()\n",
    "        \n",
    "        self.acc = sum([pred in self.new_code for pred in self.best_results])/len(self.best_results)\n",
    "        print('Accuracy for Best: ', self.acc)\n",
    "        \n",
    "        tot_preds = [pred for preds in self.results for pred in preds]\n",
    "        n_true = 0\n",
    "        for pred in tot_preds:\n",
    "            if pred in self.new_code:\n",
    "                n_true += 1\n",
    "        try:\n",
    "            self.afr = n_true/len(tot_preds)\n",
    "        except ZeroDivisionError:\n",
    "            self.afr = 0.0\n",
    "        print('Accuracy for Recommendations : ', self.afr)\n",
    "                \n",
    "    def plot_history(self):\n",
    "        hist = pd.DataFrame(self.history.history)\n",
    "        hist['epoch'] = self.history.epoch\n",
    "\n",
    "        plt.figure(figsize=(8,7))\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.plot(hist['epoch'], hist['acc'],\n",
    "               label='Train Error')\n",
    "        plt.plot(hist['epoch'], hist['loss'],\n",
    "               label = 'Train Loss')\n",
    "        plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    # in case of training big data, this is to split the data into the designated number\n",
    "    def split_data(self, num=10):\n",
    "        total_length = len(self.x)\n",
    "        total_xys = []\n",
    "        for i in range(num):\n",
    "            globals()[f'x_{i}'] = self.x[int(total_length*(1/num))*i:int(total_length*(1/num))*(i+1)]\n",
    "            globals()[f'y_{i}'] = self.y[int(total_length*(1/num))*i:int(total_length*(1/num))*(i+1)]\n",
    "            total_xys.append((globals()[f'x_{i}'], globals()[f'y_{i}']))\n",
    "        return total_xys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8169e16f-3ff9-4b1b-903e-191d3dc64c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_text = \"\"\"import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from dataset import DataSet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "66c4e887-8794-4157-85ef-6353d60ec959",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Failed to load samp_test. \n",
      ">>>Load base model instead!\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 17)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_20 (Embedding)       (None, 17, 500)      85000       ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " gru_21 (GRU)                   (None, 512)          1557504     ['embedding_20[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_25 (LSTM)                 (None, 512)          2074624     ['embedding_20[0][0]']           \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 256)          131328      ['gru_21[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_72 (Dense)               (None, 256)          131328      ['lstm_25[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 512)          0           ['dense_71[0][0]',               \n",
      "                                                                  'dense_72[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 512)         2048        ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 128)          65664       ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dense_74 (Dense)               (None, 170)          21930       ['dense_73[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,069,426\n",
      "Trainable params: 4,068,402\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 4.5127 - acc: 0.2613\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 3.0652 - acc: 0.4189\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 2.4104 - acc: 0.4730\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.9449 - acc: 0.5676\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 1.4401 - acc: 0.6532\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 1.1108 - acc: 0.7432\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.8557 - acc: 0.7973\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.6368 - acc: 0.8288\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5067 - acc: 0.8559\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.4009 - acc: 0.8649\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.3615 - acc: 0.8378\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.3344 - acc: 0.8423\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3144 - acc: 0.8604\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3053 - acc: 0.8694\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3048 - acc: 0.8649\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.2917 - acc: 0.8604\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.2754 - acc: 0.8649\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2825 - acc: 0.8829\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.2870 - acc: 0.8739\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.3031 - acc: 0.8739\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.2987 - acc: 0.8784\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3010 - acc: 0.8604\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.2867 - acc: 0.8739\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.2756 - acc: 0.8604\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2761 - acc: 0.8739\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.2683 - acc: 0.8604\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.2865 - acc: 0.8604\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.2769 - acc: 0.8694\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over samp_test.h5 file!\n"
     ]
    }
   ],
   "source": [
    "samp_model = auto_coding(new_code=samp_text,\n",
    "                      # verbose=0,\n",
    "                       model_summary=True,\n",
    "                       model_save=True,\n",
    "                       model_name='samp_test',\n",
    "                       model_load=True,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "78900580-822e-4301-97cc-d770e3d87593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t  - best match : tf\n",
      "\t - all recommendations :  ['tensorflow', 'tf']\n",
      "tup  - best match : tuple\n",
      "\t - all recommendations :  []\n",
      "p  - best match : pd\n",
      "\t - all recommendations :  ['plt', 'pd', 'pandas']\n",
      "li  - best match : list\n",
      "\t - all recommendations :  []\n",
      "d  - best match : dataset\n",
      "\t - all recommendations :  ['dic', 'dataset']\n",
      "I  - best match : Import\n",
      "\t - all recommendations :  []\n",
      "so  - best match : sort\n",
      "\t - all recommendations :  ['sort']\n",
      "m  - best match : matplotlib.pyplot\n",
      "\t - all recommendations :  []\n",
      "Accuracy for Best:  0.875\n",
      "Accuracy for Recommendations :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_test = ['t', 'tup', 'p', 'li', 'd', 'I', 'so', 'm']\n",
    "\n",
    "samp_model.auto_complete(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c5b657-aba3-4c8b-a396-01ba558073f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Failed to load tf_model. \n",
      ">>>Load base model instead!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 230s 1s/step - loss: 11.1078 - acc: 0.0126\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 8.1556 - acc: 0.0378\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 6.1463 - acc: 0.0751\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 5.1397 - acc: 0.1066\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 4.5844 - acc: 0.1388\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 4.2716 - acc: 0.1726\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 4.0541 - acc: 0.2032\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 3.8966 - acc: 0.2233\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.7719 - acc: 0.2436\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.6802 - acc: 0.2588\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.7442 - acc: 0.2592\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.5780 - acc: 0.2740\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.5217 - acc: 0.2808\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.4784 - acc: 0.2843\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.4596 - acc: 0.2854\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.4291 - acc: 0.2874\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.4041 - acc: 0.2924\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3919 - acc: 0.2911\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3778 - acc: 0.2917\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3652 - acc: 0.2926\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3498 - acc: 0.2939\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.3384 - acc: 0.2943\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.3244 - acc: 0.2939\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.3201 - acc: 0.2944\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.3120 - acc: 0.2940\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3090 - acc: 0.2937\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.2957 - acc: 0.2946\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.3160 - acc: 0.2877\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2923 - acc: 0.2895\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2784 - acc: 0.2932\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2710 - acc: 0.2947\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.2600 - acc: 0.2955\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.2536 - acc: 0.2938\n",
      "Epoch 34/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2473 - acc: 0.2941\n",
      "Epoch 35/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.2403 - acc: 0.2953\n",
      "Epoch 36/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2399 - acc: 0.2940\n",
      "Epoch 37/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.2330 - acc: 0.2951\n",
      "Epoch 38/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.2325 - acc: 0.2939\n",
      "Epoch 39/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.2339 - acc: 0.2935\n",
      "Epoch 40/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.2366 - acc: 0.2911\n",
      "Epoch 41/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2344 - acc: 0.2917\n",
      "Epoch 42/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2340 - acc: 0.2899\n",
      "Epoch 43/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.2185 - acc: 0.2922\n",
      "Epoch 44/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.2132 - acc: 0.2937\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 11.6284 - acc: 0.0180\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 7.9446 - acc: 0.0552\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 5.5433 - acc: 0.1084\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 4.4420 - acc: 0.1643\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.9682 - acc: 0.1995\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.7672 - acc: 0.2181\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.6507 - acc: 0.2276\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.5823 - acc: 0.2338\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.5372 - acc: 0.2352\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.5045 - acc: 0.2374\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4859 - acc: 0.2376\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4658 - acc: 0.2400\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.4436 - acc: 0.2381\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4350 - acc: 0.2353\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.4198 - acc: 0.2378\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4068 - acc: 0.2383\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3988 - acc: 0.2383\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3909 - acc: 0.2385\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3814 - acc: 0.2382\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3749 - acc: 0.2365\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3673 - acc: 0.2372\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3620 - acc: 0.2387\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.3543 - acc: 0.2356\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3589 - acc: 0.2367\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 10.6026 - acc: 0.0516\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 6.5527 - acc: 0.1387\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 4.5429 - acc: 0.2081\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.8082 - acc: 0.2500\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.4787 - acc: 0.2794\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3273 - acc: 0.2943\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.2491 - acc: 0.3013\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.1994 - acc: 0.3065\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.1688 - acc: 0.3084\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.1465 - acc: 0.3060\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.1337 - acc: 0.3062\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.1111 - acc: 0.3060\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0936 - acc: 0.3089\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0798 - acc: 0.3070\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.0685 - acc: 0.3097\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0564 - acc: 0.3069\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.0481 - acc: 0.3076\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0448 - acc: 0.3067\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0409 - acc: 0.3053\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0348 - acc: 0.3069\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0319 - acc: 0.3073\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0251 - acc: 0.3067\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0135 - acc: 0.3075\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.0109 - acc: 0.3071\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0108 - acc: 0.3057\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0063 - acc: 0.3046\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.0020 - acc: 0.3066\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 10.7297 - acc: 0.0558\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 7.1381 - acc: 0.0947\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 5.2238 - acc: 0.1467\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 4.3869 - acc: 0.1950\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 4.0107 - acc: 0.2256\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.8099 - acc: 0.2491\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.7099 - acc: 0.2626\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.6467 - acc: 0.2685\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.6034 - acc: 0.2717\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.5658 - acc: 0.2740\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.5476 - acc: 0.2753\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.5229 - acc: 0.2770\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.5069 - acc: 0.2769\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4893 - acc: 0.2768\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4769 - acc: 0.2758\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.4626 - acc: 0.2767\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.4540 - acc: 0.2758\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4440 - acc: 0.2761\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.4370 - acc: 0.2774\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 3.4261 - acc: 0.2763\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 227s 1s/step - loss: 3.4165 - acc: 0.2754\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 227s 1s/step - loss: 3.4088 - acc: 0.2778\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 232s 1s/step - loss: 3.4010 - acc: 0.2786\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 229s 1s/step - loss: 3.3979 - acc: 0.2752\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3920 - acc: 0.2755\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.3817 - acc: 0.2761\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.3787 - acc: 0.2754\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3776 - acc: 0.2769\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3732 - acc: 0.2749\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3684 - acc: 0.2761\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3613 - acc: 0.2763\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3610 - acc: 0.2762\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 228s 1s/step - loss: 3.3558 - acc: 0.2765\n",
      "Epoch 34/200\n",
      "182/182 [==============================] - 229s 1s/step - loss: 3.3525 - acc: 0.2743\n",
      "Epoch 35/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.3461 - acc: 0.2768\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 10.2362 - acc: 0.0840\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 7.6974 - acc: 0.1160\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 5.5919 - acc: 0.1480\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 4.7678 - acc: 0.1736\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 232s 1s/step - loss: 4.4069 - acc: 0.1914\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 4.2274 - acc: 0.2008\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 4.1218 - acc: 0.2036\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 4.0595 - acc: 0.2056\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 4.0123 - acc: 0.2079\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.9876 - acc: 0.2077\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.9642 - acc: 0.2089\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.9476 - acc: 0.2079\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.9272 - acc: 0.2094\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.9129 - acc: 0.2089\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.9079 - acc: 0.2074\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.8944 - acc: 0.2086\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.8902 - acc: 0.2085\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.8791 - acc: 0.2082\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.8736 - acc: 0.2103\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.8675 - acc: 0.2068\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.8628 - acc: 0.2081\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 3.8564 - acc: 0.2091\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.8474 - acc: 0.2074\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.8467 - acc: 0.2070\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.8405 - acc: 0.2067\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.8359 - acc: 0.2109\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 3.8348 - acc: 0.2066\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 3.8330 - acc: 0.2100\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 3.8327 - acc: 0.2067\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 234s 1s/step - loss: 3.8274 - acc: 0.2088\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 236s 1s/step - loss: 3.8211 - acc: 0.2079\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 3.8160 - acc: 0.2071\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 3.8125 - acc: 0.2076\n",
      "Epoch 34/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 3.8109 - acc: 0.2058\n",
      "Epoch 35/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 3.8070 - acc: 0.2094\n",
      "Epoch 36/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 3.8020 - acc: 0.2078\n",
      "Epoch 37/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 3.7981 - acc: 0.2098\n",
      "Epoch 38/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 3.7978 - acc: 0.2079\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 7.8974 - acc: 0.2130\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 5.2621 - acc: 0.3359\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 3.8174 - acc: 0.4106\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 2.9522 - acc: 0.4575\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 2.4571 - acc: 0.4869\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 217s 1s/step - loss: 2.1606 - acc: 0.5140\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 217s 1s/step - loss: 1.9915 - acc: 0.5348\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.8975 - acc: 0.5455\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.8535 - acc: 0.5463\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.8132 - acc: 0.5500\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.7828 - acc: 0.5533\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.7687 - acc: 0.5533\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.7602 - acc: 0.5528\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.7463 - acc: 0.5526\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.7393 - acc: 0.5550\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.7289 - acc: 0.5547\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.7236 - acc: 0.5527\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.7168 - acc: 0.5526\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.7101 - acc: 0.5530\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.7062 - acc: 0.5541\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.6994 - acc: 0.5551\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.6975 - acc: 0.5519\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.6917 - acc: 0.5545\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.6917 - acc: 0.5537\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.6868 - acc: 0.5551\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.6823 - acc: 0.5547\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.6778 - acc: 0.5541\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 1.6728 - acc: 0.5537\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 1.6762 - acc: 0.5528\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.6694 - acc: 0.5534\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 1.6667 - acc: 0.5527\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 1.6659 - acc: 0.5526\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.6632 - acc: 0.5550\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 3.9197 - acc: 0.5058\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.8179 - acc: 0.6686\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 1.2823 - acc: 0.7188\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.9914 - acc: 0.7495\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.8311 - acc: 0.7693\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.7541 - acc: 0.7802\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.7106 - acc: 0.7878\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6788 - acc: 0.7875\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6602 - acc: 0.7901\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 0.6502 - acc: 0.7889\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 0.6461 - acc: 0.7907\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 228s 1s/step - loss: 0.6361 - acc: 0.7906\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 0.6286 - acc: 0.7916\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6239 - acc: 0.7897\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6160 - acc: 0.7909\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6132 - acc: 0.7917\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6113 - acc: 0.7917\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6088 - acc: 0.7920\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6031 - acc: 0.7902\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6015 - acc: 0.7925\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6010 - acc: 0.7909\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.5959 - acc: 0.7911\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.5938 - acc: 0.7908\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 228s 1s/step - loss: 0.5925 - acc: 0.7916\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5937 - acc: 0.7907\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5888 - acc: 0.7911\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.5867 - acc: 0.7927\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.5842 - acc: 0.7928\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.5834 - acc: 0.7889\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5827 - acc: 0.7926\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5802 - acc: 0.7915\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5791 - acc: 0.7938\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5795 - acc: 0.7908\n",
      "Epoch 34/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.5761 - acc: 0.7937\n",
      "Epoch 35/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5757 - acc: 0.7909\n",
      "Epoch 36/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.5723 - acc: 0.7913\n",
      "Epoch 37/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.5719 - acc: 0.7920\n",
      "Epoch 38/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5700 - acc: 0.7931\n",
      "Epoch 39/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5693 - acc: 0.7935\n",
      "Epoch 40/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5678 - acc: 0.7933\n",
      "Epoch 41/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5641 - acc: 0.7955\n",
      "Epoch 42/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.5642 - acc: 0.7927\n",
      "Epoch 43/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.5636 - acc: 0.7907\n",
      "Epoch 44/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5641 - acc: 0.7934\n",
      "Epoch 45/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5631 - acc: 0.7927\n",
      "Epoch 46/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5621 - acc: 0.7918\n",
      "Epoch 47/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.5639 - acc: 0.7932\n",
      "Epoch 48/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.6082 - acc: 0.7836\n",
      "Epoch 49/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.6033 - acc: 0.7866\n",
      "Epoch 50/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.5723 - acc: 0.7930\n",
      "Epoch 51/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.5632 - acc: 0.7940\n",
      "Epoch 52/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 0.5572 - acc: 0.7943\n",
      "Epoch 53/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.5569 - acc: 0.7934\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 3.9628 - acc: 0.4697\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 1.7998 - acc: 0.6443\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 1.2654 - acc: 0.6964\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 1.0014 - acc: 0.7278\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.8733 - acc: 0.7415\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.8121 - acc: 0.7467\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7689 - acc: 0.7514\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7496 - acc: 0.7525\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7336 - acc: 0.7554\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7248 - acc: 0.7530\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7143 - acc: 0.7554\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7090 - acc: 0.7532\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.7077 - acc: 0.7529\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.7016 - acc: 0.7537\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.6976 - acc: 0.7545\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.6932 - acc: 0.7547\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.6891 - acc: 0.7546\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.6854 - acc: 0.7525\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.6798 - acc: 0.7552\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 0.6820 - acc: 0.7536\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 0.6775 - acc: 0.7565\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 218s 1s/step - loss: 0.6756 - acc: 0.7577\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 217s 1s/step - loss: 0.6741 - acc: 0.7548\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 219s 1s/step - loss: 0.6724 - acc: 0.7546\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 227s 1s/step - loss: 0.6674 - acc: 0.7552\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.6668 - acc: 0.7556\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 0.6647 - acc: 0.7546\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6631 - acc: 0.7549\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 0.6626 - acc: 0.7558\n",
      "Epoch 30/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 0.6638 - acc: 0.7552\n",
      "Epoch 31/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6596 - acc: 0.7564\n",
      "Epoch 32/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 0.6580 - acc: 0.7546\n",
      "Epoch 33/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 0.6578 - acc: 0.7557\n",
      "Epoch 34/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 0.6564 - acc: 0.7562\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 4.3243 - acc: 0.3811\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 2.1890 - acc: 0.5083\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.5896 - acc: 0.5671\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 1.3418 - acc: 0.5988\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.2299 - acc: 0.6087\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.1862 - acc: 0.6118\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.1549 - acc: 0.6111\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.1350 - acc: 0.6126\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.1198 - acc: 0.6140\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.1085 - acc: 0.6127\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.0966 - acc: 0.6133\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 220s 1s/step - loss: 1.0858 - acc: 0.6131\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.0796 - acc: 0.6132\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.0798 - acc: 0.6121\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.0687 - acc: 0.6124\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 223s 1s/step - loss: 1.0619 - acc: 0.6139\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.0596 - acc: 0.6126\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.0521 - acc: 0.6129\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 227s 1s/step - loss: 1.0500 - acc: 0.6133\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.0460 - acc: 0.6116\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 1.0431 - acc: 0.6117\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n",
      ">>> Load tf_model over your model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 66, 500)      60962000    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 512)          1557504     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 512)          2074624     ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          131328      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 512)         2048        ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 121924)       15728196    ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 80,652,692\n",
      "Trainable params: 80,651,668\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "==== TRAINING START ====\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 3.5203 - acc: 0.4295\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 2.0499 - acc: 0.5188\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.6507 - acc: 0.5581\n",
      "Epoch 4/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.4945 - acc: 0.5793\n",
      "Epoch 5/200\n",
      "182/182 [==============================] - 221s 1s/step - loss: 1.4316 - acc: 0.5830\n",
      "Epoch 6/200\n",
      "182/182 [==============================] - 222s 1s/step - loss: 1.4008 - acc: 0.5842\n",
      "Epoch 7/200\n",
      "182/182 [==============================] - 224s 1s/step - loss: 1.3739 - acc: 0.5834\n",
      "Epoch 8/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3534 - acc: 0.5843\n",
      "Epoch 9/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3425 - acc: 0.5837\n",
      "Epoch 10/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3292 - acc: 0.5822\n",
      "Epoch 11/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.3218 - acc: 0.5827\n",
      "Epoch 12/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3163 - acc: 0.5846\n",
      "Epoch 13/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3120 - acc: 0.5824\n",
      "Epoch 14/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3062 - acc: 0.5830\n",
      "Epoch 15/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.3004 - acc: 0.5824\n",
      "Epoch 16/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.2960 - acc: 0.5828\n",
      "Epoch 17/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.2900 - acc: 0.5849\n",
      "Epoch 18/200\n",
      "182/182 [==============================] - 225s 1s/step - loss: 1.2855 - acc: 0.5846\n",
      "Epoch 19/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2854 - acc: 0.5837\n",
      "Epoch 20/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2779 - acc: 0.5843\n",
      "Epoch 21/200\n",
      "182/182 [==============================] - 227s 1s/step - loss: 1.2766 - acc: 0.5820\n",
      "Epoch 22/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2746 - acc: 0.5822\n",
      "Epoch 23/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2700 - acc: 0.5836\n",
      "Epoch 24/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2663 - acc: 0.5829\n",
      "Epoch 25/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2649 - acc: 0.5837\n",
      "Epoch 26/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2645 - acc: 0.5836\n",
      "Epoch 27/200\n",
      "182/182 [==============================] - 226s 1s/step - loss: 1.2611 - acc: 0.5832\n",
      "Epoch 28/200\n",
      "182/182 [==============================] - 230s 1s/step - loss: 1.2590 - acc: 0.5825\n",
      "Epoch 29/200\n",
      "182/182 [==============================] - 229s 1s/step - loss: 1.2581 - acc: 0.5821\n",
      "==== TRAINING DONE ====\n",
      "Now, Load the best weights on your model.\n",
      ">>> Saved this trained model over tf_model.h5 file!\n"
     ]
    }
   ],
   "source": [
    "tf_filepath = \"../data/text_data/tf_all_symbols.txt\"\n",
    "with open(tf_filepath, 'r') as f:\n",
    "    tf_code_text = f.read()\n",
    "    \n",
    "my_model = auto_coding(new_code=tf_code_text,\n",
    "                      # verbose=0,\n",
    "                       batch_size=100,\n",
    "                       epochs=200,\n",
    "                       patience=12,\n",
    "                       model_summary=True,\n",
    "                       model_save=True,\n",
    "                       model_name='tf_model',\n",
    "                       model_load=True,\n",
    "                       data_split=True,\n",
    "                       data_split_num=10\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff33ce1-364f-4029-89df-7e5a8b346d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t  - best recommendation : tf\n",
      "\t\t - all recommendations :  ['test', 'train', 'tpu', 'tf']\n",
      "tup  - best recommendation : tuple\n",
      "\t\t - all recommendations :  ['tupe', 'tuple']\n",
      "p  - best recommendation : parse\n",
      "\t\t - all recommendations :  ['pace_to_batch_nd', 'plit', 'parse', 'pu']\n",
      "li  - best recommendation : list_variables\n",
      "\t\t - all recommendations :  ['list_variables', 'linear_to_mel_weight_matrix']\n",
      "d  - best recommendation : difference\n",
      "\t\t - all recommendations :  ['disable_with_predicate', 'difference', 'dct']\n",
      "In  - best recommendation : Int64List\n",
      "\t\t - all recommendations :  ['Int64List']\n",
      "so  - best recommendation : softmax\n",
      "\t\t - all recommendations :  ['softsign', 'softmax', 'sort']\n",
      "k  - best recommendation : kaiser_window\n",
      "\t\t - all recommendations :  ['kaiser_window', 'kaiser_bessel_derived_window']\n",
      "tf.  - best recommendation : tf.dct\n",
      "\t\t - all recommendations :  ['tf.dct', 'tf.est', 'tf.ft', 'tf.fft', 'tf.rfft2d', 'tf.ile', 'tf.fft3d', 'tf.obDef']\n",
      "Embe  - best recommendation : Embedding\n",
      "\t\t - all recommendations :  ['Embedding', 'EmbeddingGradients', 'Embeedding', 'Embect']\n",
      "L  - best recommendation : LoadOptions\n",
      "\t\t - all recommendations :  ['LossScale', 'LoadOptions']\n",
      "act  - best recommendation : actter_nd\n",
      "\t\t - all recommendations :  ['acturate_cast', 'actter_nd', 'acts']\n"
     ]
    }
   ],
   "source": [
    "x_test = ['t', 'tup', 'p', 'li', 'd', 'In', 'so', 'k', 'tf.', 'Embe', 'L', 'act', ]\n",
    "\n",
    "my_model.auto_complete(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73812b29-b670-499f-aedc-9b01e4a28594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['test', 'train', 'tpu', 'tf'],\n",
       " ['tupe', 'tuple'],\n",
       " ['pace_to_batch_nd', 'plit', 'parse', 'pu'],\n",
       " ['list_variables', 'linear_to_mel_weight_matrix'],\n",
       " ['disable_with_predicate', 'difference', 'dct'],\n",
       " ['Int64List'],\n",
       " ['softsign', 'softmax', 'sort'],\n",
       " ['kaiser_window', 'kaiser_bessel_derived_window'],\n",
       " ['tf.dct',\n",
       "  'tf.est',\n",
       "  'tf.ft',\n",
       "  'tf.fft',\n",
       "  'tf.rfft2d',\n",
       "  'tf.ile',\n",
       "  'tf.fft3d',\n",
       "  'tf.obDef'],\n",
       " ['Embedding', 'EmbeddingGradients', 'Embeedding', 'Embect'],\n",
       " ['LossScale', 'LoadOptions'],\n",
       " ['acturate_cast', 'actter_nd', 'acts']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a0a805-b0d1-4f44-bcc2-852c272761ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842969a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
